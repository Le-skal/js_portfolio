{
  "projects": {
    "Scraping-boulanger": {
      "title": "Scraping-boulanger",
      "description": "Extracts, stores, cleans, and visualizes product data from Boulanger for market insights.",
      "metadata": {
        "role": "Data Analyst",
        "category": "Data Extraction, Web Scraping, Data Engineering",
        "timeline": "March 2025",
        "liveUrl": "null",
        "githubUrl": "https://github.com/Le-skal/Scraping-boulanger"
      },
      "overview": "This project develops a full-lifecycle data pipeline to extract, store, clean, and visualize product data from Boulanger, a French electronics retailer. It transforms unstructured web data into actionable intelligence, showcasing an end-to-end data analytics workflow using Python and various libraries.",
      "challenge": {
        "problem": "E-commerce data is dynamic, unstructured, and difficult to access for systematic analysis. Manual collection is impractical, and technical hurdles like dynamically loaded content and anti-scraping measures prevent direct analytical access. The core problem was efficient, reliable data extraction and organization.",
        "goal": "To build an automated, modular system for web scraping, storing data in a database, cleaning/normalizing it, and generating meaningful visualizations. The project aims to provide data-driven insights for market analysis and decision-making about Boulanger's product offerings.",
        "constraints": "Overcoming anti-scraping mechanisms, handling JavaScript-rendered content, and managing diverse data formats were technical hurdles. Ensuring data integrity, freshness, and a user-friendly pipeline interface added further complexity."
      },
      "discovery": {
        "requirements": "The project needed a flexible scraping and cleaning approach for specific product types (laptops, consoles, phones, TVs) and their unique attributes. Key analytical outcomes included average ratings, price comparisons, and popularity identification, guiding visualization design.",
        "competitiveAnalysis": "The project implicitly serves as a self-sufficient market intelligence tool, allowing monitoring of competitor offerings and trends internally. This bypasses costly third-party data or time-consuming manual research efforts.",
        "technicalResearch": "Python was chosen for its data science ecosystem. `requests` and `BeautifulSoup4` were selected for static HTML, `Selenium` for dynamic JavaScript content, and `MySQL` for robust data storage. `pandas`, `numpy`, `matplotlib`, `seaborn` were chosen for data manipulation and visualization, with `python-dotenv` for secure credentials."
      },
      "architecture": {
        "informationArchitecture": "The project has a modular design with `main.py` as the central CLI orchestrator. Directories like `scraping/`, `nettoyage/`, `config/`, and `visualisation/` separate concerns, supporting a clear data flow: Scrape -> Store -> Clean -> Visualize.",
        "technicalDecisions": "Key decisions included a hybrid scraping approach (`requests`/`BeautifulSoup4` + `Selenium`) for comprehensive data capture, MySQL for relational database persistence with table recreation for freshness, and category-specific data cleaning for accurate normalization. An interactive CLI simplified user control, and `python-dotenv` managed secure environment configuration."
      },
      "developmentProcess": {
        "phase1": "Established project infrastructure, repository, and core folder structure (`scraping/`, `nettoyage/`, `config/`). Configured the Python environment with all dependencies using `requirements.txt` and `environment.yml`. Developed basic database connection and initial `main.py` CLI menu.",
        "phase2": "Implemented web scraping logic in `scraping/` using the hybrid approach to extract product details and integrate database storage into specific MySQL tables. Built modular, category-specific data cleaning functions in `nettoyage/` using `pandas`. Developed data visualization capabilities with `matplotlib` and `seaborn` for insights.",
        "phase3": "Refined scraping selectors, added error handling, and optimized cleaning algorithms for accuracy and resilience. Enhanced the interactive CLI for better usability and feedback. Reviewed database interactions and ensured consistent data types throughout the pipeline for robustness."
      },
      "keyFeatures": [
        {
          "title": "Comprehensive Web Scraping (Hybrid Approach)",
          "description": "Extracts detailed product information (name, price, rating, reviews, availability) from Boulanger across various categories.",
          "implementation": "`scraping.py` uses `requests` and `BeautifulSoup4` for static HTML, while `Selenium` controls a headless browser for dynamic JavaScript content. Random delays are incorporated to mimic human browsing.",
          "challenges": "Successfully navigated complex e-commerce structures, handled dynamically loaded content, and reduced detection by anti-scraping measures."
        },
        {
          "title": "Robust MySQL Database Integration",
          "description": "Stores all extracted and structured product data in a MySQL database for persistent storage and complex query analysis.",
          "implementation": "`mysql-connector-python` manages connections, dynamically creating and managing tables per category. Tables are dropped and recreated before each scrape to ensure data freshness.",
          "challenges": "Designed appropriate schemas for diverse product data, secured connections with `python-dotenv`, and ensured efficient, integrity-preserving data insertion."
        },
        {
          "title": "Modular and Category-Specific Data Cleaning",
          "description": "Cleans, transforms, and standardizes raw scraped data, tailored for each product category, preparing it for analysis.",
          "implementation": "Separate `nettoyage/` modules (e.g., `nettoyage_ordinateur.py`) use `pandas` functions for handling missing values, extracting numerical data, standardizing units, and converting data types.",
          "challenges": "Addressed inherent inconsistencies and varied web-scraped data formats, developed precise parsing rules for unique product attributes, ensuring efficient and robust cleaning."
        },
        {
          "title": "Interactive Command-Line Interface (CLI)",
          "description": "Provides a user-friendly, menu-driven interface to interact with the entire data pipeline, guiding users through scraping, cleaning, and visualization.",
          "implementation": "The `main.py` script presents a menu, handles user input, and orchestrates calls to relevant functions from `scraping/`, `nettoyage/`, and `visualisation/` modules.",
          "challenges": "Created an intuitive and logical flow for a multi-stage project, ensured robust input validation, and provided clear user feedback at each step."
        },
        {
          "title": "Insightful Data Visualization",
          "description": "Generates a range of interactive graphs and charts from cleaned data, offering clear insights into product performance and market trends.",
          "implementation": "Utilizes `pandas` for data preparation and `matplotlib`/`seaborn` to create plots like average ratings, price comparisons, popular product identification, and price-rating correlations.",
          "challenges": "Chose effective visualization types for different insights, handled potential outliers, and ensured graphs were clear, aesthetic, and actionable."
        }
      ],
      "testing": "Testing was integrated across the pipeline: manual checks on raw scraped data in MySQL validated extraction accuracy, leading to iteration on CSS selectors. The cleaning phase involved rigorous inspection of processed data for correct handling of values, types, and standardization, driving script refinements. Visualization provided a crucial visual feedback loop, with unexpected chart patterns indicating upstream data quality issues. The ability to drop and recreate tables facilitated rapid iteration and re-validation of the entire pipeline.",
      "results": {
        "technicalAchievements": "Successfully implemented an end-to-end Python data pipeline, demonstrating proficiency in web scraping, database management, cleaning, and visualization. Developed a robust hybrid scraping solution for dynamic content and achieved modularity through category-specific cleaning routines. Secured credentials with `python-dotenv` and delivered an intuitive CLI for complex pipeline management.",
        "businessImpact": "Provides a powerful market intelligence tool for analyzing Boulanger's product offerings, pricing, and customer satisfaction. Offers data-driven insights to inform inventory, marketing, or competitive positioning. Automates time-consuming data collection, providing timely access to critical market information.",
        "personalGrowth": "Deepened expertise in advanced hybrid web scraping techniques and strengthened relational database design/management skills with MySQL. Mastered `pandas` for handling messy real-world data and enhanced data visualization proficiency with `matplotlib`/`seaborn`. Gained valuable experience building a complete, multi-stage data project from conception to actionable output."
      },
      "techStack": {
        "frontend": "N/A (CLI-based, no specific web frontend)",
        "backend": "Python, MySQL",
        "tools": "pip, conda, python-dotenv",
        "libraries": "requests, BeautifulSoup4, Selenium, mysql-connector-python, pandas, numpy, matplotlib, seaborn"
      },
      "learnings": [
        "Mastered combining `requests`/`BeautifulSoup4` with `Selenium` for effective hybrid scraping of modern, dynamic websites, understanding when and how to integrate each tool.",
        "Understood the importance of modular data pipelines, improving code organization, maintainability, and enabling independent development and testing for scalable data projects.",
        "Implemented category-specific cleaning and table recreation, demonstrating best practices for ensuring data accuracy, consistency, and freshness in evolving web environments.",
        "Reinforced security and portability in development by adopting `python-dotenv` for managing sensitive credentials, avoiding hardcoded critical information."
      ],
      "futureEnhancements": [
        "Implement automated scheduling using `cron` or `Apache Airflow` for regular execution of the scraping and analysis pipeline.",
        "Integrate robust error logging and alerting (e.g., email/SMS) for scrape failures, cleaning issues, or database errors.",
        "Develop a web-based dashboard using Streamlit, Flask, or Dash for interactive data visualization beyond static plots.",
        "Expand scraping capabilities to cover a broader range of product categories on Boulanger, requiring new logic and cleaning rules.",
        "Implement proxy rotation and CAPTCHA handling solutions to enhance scraping resilience against anti-bot measures."
      ],
      "conclusion": "The 'Scraping-boulanger' project showcases Python's power in building comprehensive data solutions, demonstrating mastery of the entire data lifecycle. It effectively transforms unstructured web data into actionable business intelligence, highlighting technical prowess in web scraping, database management, analytics, and visualization. This robust tool delivers invaluable insights from the digital landscape."
    },
    "TripHackathon": {
      "title": "TripWise",
      "description": "TripWise: AI-powered travel planning with personalized itineraries via a local LLM.",
      "metadata": {
        "role": "Full-Stack Developer",
        "category": "Web Development, AI Implementation",
        "timeline": "May 2025 - October 2025",
        "liveUrl": null,
        "githubUrl": "https://github.com/Le-skal/TripHackathon"
      },
      "overview": "TripWise is an AI-powered web app leveraging React, Node.js, Firebase, and Ollama to create personalized travel itineraries. It addresses time-consuming research and overwhelming choices, providing a seamless platform for secure authentication and AI-driven trip management. This project exemplifies integrating advanced AI into a user-friendly SaaS platform, showcasing robust full-stack development.",
      "challenge": {
        "problem": "Traditional travel planning involves extensive manual research, leading to time-consuming, inefficient, and rarely personalized itineraries. Users need a more intelligent, automated, and convenient planning method.",
        "goal": "Develop a full-stack web application with integrated AI for streamlined travel planning. This included secure authentication, user-defined trip parameters, and LLM-generated comprehensive, personalized itineraries.",
        "constraints": "Key technical challenges involved integrating a local LLM (Ollama) into a web backend, managing communication protocols, and ensuring performance. Robust, scalable user authentication (Firebase) and a fluid React UI also presented architectural challenges, requiring a modular codebase for future enhancements."
      },
      "discovery": {
        "requirements": "User needs highlighted a desire for automated, detailed travel plans, including secure login, trip parameter input (destination, duration, budget), AI generation, and a trip management dashboard. A 5-day Bali trip example guided AI interaction design.",
        "competitiveAnalysis": "Existing tools are often static or rely on simple filters. TripWise aimed to differentiate with dynamic, AI-generated itineraries tailored to individual inputs, providing personalized content instantly, a significant upgrade from manual planning.",
        "technicalResearch": "React was chosen for its component-based architecture and ecosystem. Firebase offered ease of integration for Auth and Firestore for scalable NoSQL data. Node.js/Express provided an efficient backend for API requests. Ollama was selected for local LLM integration, offering control and potential cost savings over cloud alternatives."
      },
      "architecture": {
        "informationArchitecture": "The project uses a decoupled architecture: React frontend (Port 3000) for UI, Node.js/Express backend (Port 5000) for API and Ollama communication, Firebase for Auth/Firestore, and Ollama (local LLM) for AI. This enables independent development and managed services.",
        "technicalDecisions": "Axios facilitates client-server and backend-LLM HTTP requests, with CORS configured for cross-origin security. Firebase Firestore provides flexible NoSQL data persistence and real-time sync. Dotenv manages environment variables securely. Direct Ollama integration through the Node.js backend ensures robust AI processing tailored to application needs."
      },
      "developmentProcess": {
        "phase1": "Focused on core structure setup: initializing React and Node.js/Express. Firebase Authentication was integrated early for secure user access, followed by Firestore setup for basic data storage, establishing essential user management and data persistence.",
        "phase2": "Involved building core functionalities. The Node.js backend developed the `/api/generate-plan` endpoint to communicate with Ollama. Frontend `CreateTrip` collected user inputs, `Dashboard` and `TripList` displayed itineraries, and user profile management was integrated.",
        "phase3": "Concentrated on UI refinement, basic error handling, and iterative prompt engineering for Ollama to improve itinerary accuracy and creativity. This phase established a solid foundation for future features and extensive testing, though the project remains in development."
      },
      "keyFeatures": [
        {
          "title": "Secure User Authentication",
          "description": "Enables secure user registration and login to access personalized travel plans.",
          "implementation": "Uses Firebase Authentication, with the React frontend interacting via Firebase SDKs for sign-up, sign-in, and session management, abstracting credential handling and security.",
          "challenges": "Simplified user credential management and session handling, allowing focus on core features rather than complex security logic."
        },
        {
          "title": "AI-Powered Itinerary Generation",
          "description": "Generates detailed, day-by-day itineraries based on user inputs like destination, duration, and budget.",
          "implementation": "React frontend sends user inputs to the Node.js backend's `/api/generate-plan` endpoint. The backend uses Axios to prompt the local Ollama LLM, which returns structured JSON for frontend display and storage.",
          "challenges": "Successfully integrated a local LLM, managed asynchronous AI processing, structured LLM output for application use, and refined prompts for relevant plans."
        },
        {
          "title": "Comprehensive Trip Management",
          "description": "Provides a dashboard for users to view, organize, and manage all their created travel itineraries.",
          "implementation": "Firebase Firestore stores all trip data linked to user accounts. The React frontend directly queries Firestore to retrieve and display `TripList` and `Dashboard` views, leveraging Firestore's real-time capabilities.",
          "challenges": "Designed a flexible Firestore schema for diverse trip details and complex AI-generated itineraries, ensuring efficient data retrieval and display."
        },
        {
          "title": "Intuitive User Interface",
          "description": "Offers a clean, responsive, and easy-to-navigate interface for creating trips, viewing itineraries, and managing profiles.",
          "implementation": "Developed with React using a component-based design for reusability (SignIn, Dashboard, CreateTrip, etc.). Styling uses CSS to ensure a consistent, modern aesthetic and cross-browser compatibility.",
          "challenges": "Created an engaging UX for data input and visualization, maintaining cross-browser compatibility and a clean aesthetic."
        }
      ],
      "testing": "Functional testing was primarily manual, verifying authentication flows, `CreateTrip` form submissions, and AI-generated itinerary accuracy. Iterative prompt refinement for the LLM was continuous. Though 'In Development,' future plans include automated unit tests using Jest and React Testing Library for robust code quality.",
      "results": {
        "technicalAchievements": "TripWise successfully integrated a local LLM (Ollama) into a full-stack web application, showcasing cutting-edge AI implementation. It robustly utilizes Firebase for authentication and NoSQL data management, and the React frontend delivers a dynamic, component-driven UX, orchestrating complex interactions across client, backend, Firebase, and AI engine.",
        "businessImpact": "As an 'In Development' application, TripWise demonstrates significant potential to disrupt travel planning by offering an AI-driven solution. It aims to save users time and effort through personalized, automated itineraries, positioning it as a valuable SaaS product with future monetization potential via premium features.",
        "personalGrowth": "This project offered invaluable experience in architecting complex full-stack applications. I deepened expertise in integrating diverse technologies, particularly local LLMs, honed Node.js, Firebase, and React skills, and gained practical insight into prompt engineering and AI-driven feature development."
      },
      "techStack": {
        "frontend": "React, CSS",
        "backend": "Node.js (v20+), Express (v5.1.0), CORS (v2.8.5), Body Parser (v2.2.0), Axios (v1.9.0), Dotenv (v16.5.0)",
        "tools": "Firebase (Authentication, Firestore), Ollama (Local LLM)",
        "libraries": "Axios (HTTP client for backend-LLM communication), Dotenv (environment variables), Body Parser (request body parsing), CORS (cross-origin requests)."
      },
      "learnings": [
        "Seamless LLM Integration: Successfully integrating a local LLM like Ollama showcased the potential for customized, cost-effective AI solutions, requiring careful backend API design and precise prompt engineering.",
        "Power of BaaS for Rapid Development: Leveraging Firebase for authentication and database significantly accelerated development, allowing focus on core logic and AI integration over complex infrastructure.",
        "Architectural Clarity in Complex Systems: Maintaining clear separation of concerns between frontend, backend, Firebase, and Ollama was crucial, with the architectural diagram proving invaluable for managing interconnected components.",
        "Importance of a Strong Frontend Foundation: Building a robust, modular React frontend with reusable components was essential for managing complexity, ensuring a smooth user experience, and facilitating future feature expansions."
      ],
      "futureEnhancements": [
        "Enhanced User Database: Create a dedicated `/users` collection in Firestore for more detailed user profiles and preferences.",
        "Trip Sharing: Implement functionality for users to share their personalized itineraries with friends and family.",
        "Google Maps Integration: Integrate Google Maps to visualize itineraries, showing routes, points of interest, and directions.",
        "PDF Export: Provide an option to generate and export trip itineraries as printable PDF documents.",
        "Unit & Integration Tests: Introduce comprehensive testing with Jest and React Testing Library for code quality and stability."
      ],
      "conclusion": "TripWise represents a significant achievement, blending full-stack engineering with cutting-edge AI to provide an intelligent, personalized, and efficient travel planning solution. It showcases strong command of diverse technologies and the ability to conceptualize, design, and build a compelling SaaS product, poised to redefine the travel planning experience."
    },
    "ABDD": {
      "title": "ABDD: Wine Dataset Analysis",
      "description": "A comprehensive ML pipeline for classifying and clustering the Wine dataset with advanced analysis and visualizations.",
      "metadata": {
        "role": "Data Scientist",
        "category": "Machine Learning, Clustering Analysis, Classification",
        "timeline": "October 2025 (10 days intensive)",
        "liveUrl": "null",
        "githubUrl": "https://github.com/Le-skal/ABDD"
      },
      "overview": "This project implements and evaluates KNN, K-Means, and Hierarchical Clustering on the Wine dataset. It surpasses basic requirements with 8 advanced analyses, 25+ visualizations, and a comparative study. Emphasizing production-ready code, documentation, and reproducibility, it demonstrates robust data science skills.",
      "challenge": {
        "problem": "The challenge was to thoroughly analyze the Wine dataset by implementing, evaluating, and comparing machine learning algorithms. This required advanced techniques, reproducibility, and comprehensive visualization beyond standard model training.",
        "goal": "Deliver a complete ML pipeline for wine classification and clustering using KNN, K-Means, and CAH. The objective was to integrate 8 advanced analyses and 25+ visualizations within a production-ready framework to offer exceptionally detailed insights.",
        "constraints": "Academic rigor for an ECE B3 Data Mining course mandated deep theoretical understanding and bug-free implementation. Reproducible results, comprehensive documentation, and production-ready code also imposed high standards within a tight development timeline."
      },
      "discovery": {
        "requirements": "The project's academic context and aim for '8 Advanced Analyses' indicated a need to delve beyond basic implementation. The developer focused on understanding algorithm mechanics, performance drivers, and thorough outcome evaluation, including metrics like ROC curves and cluster stability.",
        "competitiveAnalysis": "The ambition for '8 Advanced Analyses' and '25+ Visualizations' suggests a desire to create a standout portfolio piece. It aimed to differentiate by offering professional-level depth, reproducibility, and visual storytelling, uncommon in typical academic assignments.",
        "technicalResearch": "Extensive research was conducted on algorithm implementations (KNN distance, K-Means initialization, CAH linkage), advanced evaluation metrics (ARI, NMI, ROC AUC, silhouette scores), effective visualization techniques, and best practices for ensuring reproducibility across libraries."
      },
      "architecture": {
        "informationArchitecture": "The project is logically structured around KNN, K-Means, and CAH, likely with dedicated modules for each, ensuring clear separation of concerns. A dedicated section for comparative visualizations further enhances analysis and maintainability.",
        "technicalDecisions": "Python and scikit-learn were chosen for their robust ML ecosystem. Fixed random seeds were critical for reproducible results. An extensive visualization suite (25+ plots) was planned for clear communication, and a documentation-first approach ensured high code quality and clarity."
      },
      "developmentProcess": {
        "phase1": "Setup involved configuring the Python environment, installing libraries, and loading the Wine dataset. Initial data exploration, basic preprocessing, and establishing a robust project structure with dedicated directories were completed.",
        "phase2": "Core implementation included KNN with 10-Fold CV, ROC curves, and learning curves. K-Means incorporated Elbow/Silhouette methods for optimal k. CAH featured various linkage methods and dendrograms. Advanced analyses and 25+ visualizations were integrated across all algorithms.",
        "phase3": "The final phase focused on refining the project to 'Production-Ready Code' standards. This involved comprehensive documentation, error handling, fine-tuning visualizations, and rigorous testing for reproducibility via fixed random seeds, leading to an 'Enhanced Edition (Version 2.0)'."
      },
      "keyFeatures": [
        {
          "title": "K-Nearest Neighbors (KNN) with Advanced Evaluation",
          "description": "Implements a supervised classifier to predict wine origin, focusing on optimal `k` selection and thorough performance assessment beyond simple accuracy.",
          "implementation": "Leverages `scikit-learn` for KNN, employing 10-Fold Cross-Validation for `k` tuning. Generates ROC Curves & AUC for multi-class insights, Learning Curves, and Permutation-based Feature Importance.",
          "challenges": "Determining the ideal `k` value without overfitting in a multi-class setting, and ensuring evaluation metrics provided actionable insights into model performance."
        },
        {
          "title": "K-Means Clustering with Optimal K Determination",
          "description": "Groups similar wine samples without prior labels, critically determining the optimal number of clusters (`k`) robustly.",
          "implementation": "Uses `scikit-learn` K-Means, evaluating `k` via the Elbow Method (inertia) and Silhouette Scores. Generates Silhouette Histograms and projects clusters onto PCA for 2D visualization with centroids.",
          "challenges": "Overcoming the inherent difficulty in objectively defining the 'correct' number of clusters in unsupervised learning by combining multiple data-driven approaches."
        },
        {
          "title": "Hierarchical Clustering (CAH) with Detailed Dendrograms",
          "description": "Builds a hierarchy of clusters, represented by a dendrogram, allowing flexible interpretation and validation of cluster structure.",
          "implementation": "Implements agglomerative hierarchical clustering, exploring various linkage methods. Generates detailed Dendrograms, Silhouette Histograms, and analyzes Cluster Stability using resampling techniques like Bootstrap with ARI.",
          "challenges": "Interpreting complex dendrograms to derive meaningful clusters. Stability analysis and PCA projections provided context and validated the discovered structures."
        },
        {
          "title": "Comprehensive Visualization Suite (25+ Plots)",
          "description": "Offers an extensive array of visual aids to interpret, evaluate, and compare the performance of all implemented machine learning models.",
          "implementation": "Utilizes `Matplotlib` and `Seaborn` to generate diverse plots, including confusion matrices, ROC curves, learning curves, dendrograms, elbow/silhouette plots, and PCA projections for all algorithms.",
          "challenges": "Designing clear, informative, and visually appealing plots that effectively communicate complex statistical results to both technical and non-technical audiences, ensuring consistent style."
        },
        {
          "title": "Cross-Method Comparative Analysis",
          "description": "Directly compares performance, characteristics, and computational efficiency of KNN, K-Means, and CAH on the Wine dataset.",
          "implementation": "Synthesizes results from individual analyses, generating plots for Metrics Comparison (accuracy, ARI, NMI), Execution Time, and Side-by-Side PCA visualizations to contrast data segmentation.",
          "challenges": "Standardizing comparison metrics across supervised and unsupervised tasks, and presenting diverse findings in an easily digestible, insightful manner."
        }
      ],
      "testing": "The project ensures quality through fixed random seeds for reproducibility, guaranteeing consistent results. KNN's 10-Fold Cross-Validation robustly assesses model stability and generalization. Cluster stability analysis, using ARI distribution, validates the robustness of unsupervised solutions, while iterative refinement (Version 2.0) indicates continuous improvement.",
      "results": {
        "technicalAchievements": "Successfully implemented and extensively evaluated KNN, K-Means, and CAH, integrating 8 advanced analyses and 25+ visualizations. Achieved production-ready code with comprehensive documentation and ensured reproducible results via fixed random seeds, alongside detailed comparative analysis.",
        "businessImpact": "This project significantly elevates the developer's academic portfolio, demonstrating mastery of ML and advanced analytical techniques. It serves as a compelling example of thoroughness, suitable as a foundation for further research or future data analysis projects.",
        "personalGrowth": "The developer deepened understanding of ML theory and application, mastered advanced evaluation metrics, and honed data visualization skills. Gained experience in structuring complex projects for maintainability, reproducibility, and adopted an iterative development mindset."
      },
      "techStack": {
        "frontend": "N/A",
        "backend": "Python (3.8+)",
        "tools": "Git, GitHub",
        "libraries": "scikit-learn (1.0+) (ML algorithms, preprocessing), NumPy (numerical ops), Pandas (data manipulation), Matplotlib (plotting base), Seaborn (statistical visualization)"
      },
      "learnings": [
        "Moving beyond basic metrics to advanced evaluations (ROC AUC, ARI, NMI, cluster stability) provides a nuanced and trustworthy understanding of model performance and robustness.",
        "Generating a diverse suite of 25+ visualizations taught the power of visualization as an integral part of discovery and interpretation, revealing hidden insights.",
        "Explicitly incorporating fixed random seeds and robust cross-validation methods highlighted the paramount value of reproducibility in data-driven projects.",
        "The iterative approach and clear organization reinforced the benefits of structured, well-documented project development for maintainability and scalability."
      ],
      "futureEnhancements": [
        "Explore other clustering algorithms like DBSCAN, GMM, or Spectral Clustering for broader comparison.",
        "Integrate a basic Neural Network for classification to compare its performance against traditional ML models.",
        "Adapt the pipeline to analyze other publicly available datasets (e.g., Iris, breast cancer) to demonstrate versatility.",
        "Create an interactive web dashboard using tools like Streamlit or Dash for dynamic exploration of results.",
        "Implement more advanced hyperparameter tuning techniques such as GridSearchCV, RandomizedSearchCV, or Bayesian optimization."
      ],
      "conclusion": "The 'ABDD: Wine Dataset Analysis' project showcases a data scientist's comprehensive ability to implement, rigorously evaluate, deeply analyze, and compellingly visualize ML algorithms. Its commitment to production-ready code, thorough documentation, and reproducible results underscores a professional approach crucial for tackling complex data challenges."
    },
    "PublicTradeBot": {
      "title": "Trade Bot",
      "description": "AI-powered automated trading system for crypto and French stocks via ML/NLP.",
      "metadata": {
        "role": "Data Scientist / ML Engineer",
        "category": "Web Development - ML Implementation, FinTech, Automated Trading",
        "timeline": "Late 2024 - Present (Ongoing Development)",
        "liveUrl": "https://tradebotskals.netlify.app",
        "githubUrl": "https://github.com/Le-skal/PublicTradeBot"
      },
      "overview": "PublicTradeBot is an AI-powered automated trading system for cryptocurrencies and French stocks. It leverages cutting-edge ML and NLP to analyze 250+ daily L'Agefi articles, generating actionable trading signals. The project focuses on a lean, serverless architecture with GitHub Actions and Google Sheets for data storage, combining sentiment analysis with technical indicators and XGBoost for highly selective decisions.",
      "challenge": {
        "problem": "Individual traders struggle to process vast financial news, market data, and execute trades efficiently, often suffering from emotional biases and time commitments. Existing automated solutions are complex, expensive, or lack customization for specific markets and data sources.",
        "goal": "Develop a fully automated, AI-driven trading system to generate high-confidence trade signals and execute them autonomously. The system needed to process diverse data, apply advanced ML, incorporate robust risk management, and provide transparent tracking with a lean, easy-to-deploy architecture.",
        "constraints": "Reliably fetching 250+ daily L'Agefi articles and market data was crucial, requiring daily real-time processing. The XGBoost model needed strong backtested performance, and the entire system demanded serverless automation via GitHub Actions with Google Sheets for data storage. Secure API key handling was paramount."
      },
      "discovery": {
        "requirements": "The project required timely data acquisition, robust analytical methods combining sentiment and technical indicators, predictive modeling, disciplined execution, and transparent performance monitoring. Key specifics included French news processing, crypto/stock support, and risk-controlled strategies.",
        "competitiveAnalysis": "Unlike many bots relying on simple rules or technical indicators, PublicTradeBot integrates advanced NLP for French news with ML for nuanced signal generation. Its 'no-database' Google Sheets approach also offers a simpler, more accessible alternative to complex enterprise systems.",
        "technicalResearch": "Research led to CamemBERT for French financial sentiment, XGBoost for predictive modeling, and GitHub Actions for serverless automation. Google Sheets API was chosen for lightweight data persistence, and various risk management techniques were explored for strategy implementation."
      },
      "architecture": {
        "informationArchitecture": "The system features a modular, sequential pipeline separating training and production components. The daily bot uses `8_daily_runner.py` for orchestration, interacting with `google_sheets_client.py` and a pre-trained `trading_model.pkl`. The training pipeline includes scripts for data fetching, sentiment, technical indicators, feature engineering, model training, and backtesting.",
        "technicalDecisions": "Python was chosen for its data science ecosystem and scripting ease. GitHub Actions provides free, serverless daily execution. Google Sheets offers simple, 'no-database' persistence. XGBoost was selected for efficient, accurate signal generation, while CamemBERT ensures high-quality French sentiment analysis. A modular design enhances maintainability."
      },
      "developmentProcess": {
        "phase1": "This phase established the core Python environment, repository, and secure `.env` setup. Crucially, Google Sheets integration via a service account and `google_sheets_client.py` was prioritized for reliable data operations. A basic GitHub Actions workflow was prototyped for daily script execution.",
        "phase2": "Core functional components were developed: data fetching for RSS and market prices, CamemBERT-based sentiment analysis, and technical indicator generation. These were combined into features for the XGBoost model, which was trained and saved. A backtesting script validated the strategy, and `8_daily_runner.py` integrated all production-ready logic for market fetching, prediction, and trade execution.",
        "phase3": "Rigorous backtesting against 2024-2025 data refined strategy parameters and the ML confidence threshold. Error handling was improved, and the GitHub Actions workflow optimized for robust daily execution and logging. Security measures using GitHub Secrets were implemented, and an optional Vercel dashboard for performance visualization was planned."
      },
      "keyFeatures": [
        {
          "title": "Automated Daily Trading Pipeline",
          "description": "The bot autonomously executes its trading strategy daily, fetching new data, generating signals, and potentially placing trades without manual intervention.",
          "implementation": "Orchestrated by GitHub Actions, `8_daily_runner.py` runs daily at 12 PM CET, fetching L'Agefi articles and market prices. It uses the pre-trained XGBoost model for predictions, applies defined risk management, and logs all activities to Google Sheets.",
          "challenges": "Ensuring robust error handling for external APIs, managing state, and preventing duplicate trades in a scheduled, serverless environment required careful design of logging and checking mechanisms."
        },
        {
          "title": "Advanced French Sentiment Analysis",
          "description": "Analyzes the sentiment of French financial news articles from L'Agefi to gauge market mood for specific assets.",
          "implementation": "`3_sentiment_analysis.py` (integrated into the daily runner) uses a pre-trained CamemBERT transformer model to process raw text from RSS feeds. It extracts nuanced sentiment scores relevant to the trading universe.",
          "challenges": "Sourcing and integrating a suitable large NLP model for French financial jargon into an automated pipeline, considering resource constraints, and ensuring efficient inference for daily operations were key."
        },
        {
          "title": "Machine Learning Signal Generation (XGBoost)",
          "description": "Generates precise trading signals by combining sentiment, technical indicators, and other features, predicting optimal trade directions.",
          "implementation": "`6_train_model.py` trains an XGBoost classifier on historical data with features from sentiment and technical indicators. The `trading_model.pkl` is then used by the daily runner to produce a 'buy', 'sell', or 'hold' signal, only if a 0.520 confidence threshold is met.",
          "challenges": "Balancing model complexity, interpretability, and training speed was crucial. Preventing overfitting through rigorous feature selection and hyperparameter tuning, and defining a meaningful confidence threshold for high-conviction trades, were also challenging."
        },
        {
          "title": "\"No-Database\" Data Management with Google Sheets",
          "description": "Stores all trading history, performance metrics, news articles, and operational data in Google Sheets, eliminating the need for a traditional database.",
          "implementation": "`google_sheets_client.py` provides an API wrapper, handling authentication via a Google Service Account (GitHub Secrets). It appends new trade records, updates portfolio status, and retrieves historical data, with each sheet acting as a structured table.",
          "challenges": "Designing an efficient Google Sheets schema for various data types, ensuring secure access via service accounts, and preventing accidental data corruption through controlled permissions and robust client-side logic were addressed."
        },
        {
          "title": "Robust Risk Management & Strategy Implementation",
          "description": "Enforces strict rules to limit potential losses and manage overall portfolio exposure, ensuring disciplined trading.",
          "implementation": "The strategy hardcodes rules: 25% capital per trade, max 3 open positions, 8% stop-loss, 20% take-profit, and max 1 trade per day. These are integrated into the `8_daily_runner.py` execution logic and monitored via Google Sheets-tracked portfolio data.",
          "challenges": "Dynamically integrating these rules with real-time market data and trade execution calls, ensuring correct identification and action upon stop-loss/take-profit conditions across multiple open positions, required careful implementation."
        }
      ],
      "testing": "Modular unit testing for each ML pipeline component ensured correct functionality and data integrity. A dedicated `7_backtest.py` script rigorously evaluated the strategy on 2024-2025 historical data, allowing iterative refinement of the ML model and risk parameters. The full daily runner was integration tested in a simulated environment, verifying the end-to-end flow. Post-deployment, Google Sheets provided continuous performance monitoring for feedback.",
      "results": {
        "technicalAchievements": "Successfully implemented an end-to-end, fully automated ML-driven trading system for daily execution. Integrated advanced CamemBERT NLP for French financial news sentiment, and developed a robust data pipeline transforming diverse sources into ML-ready features. Achieved serverless, cost-effective automation via GitHub Actions, and a creative 'no-database' solution using Google Sheets. Backtested performance on 2024-2025 data yielded strong results.",
        "businessImpact": "Provided a systematic, unbiased trading approach, mitigating human error and emotional biases. Enabled efficient and scalable market analysis by automating processing of hundreds of articles and market data daily. Offered a cost-effective solution through lean architecture and transparently logged all trades for accountability and continuous improvement.",
        "personalGrowth": "Deepened expertise in practical ML/NLP for FinTech and gained significant experience in MLOps, including designing and deploying automated ML pipelines with serverless tech. Enhanced skills in data engineering, API integration, and fault-tolerant Python applications. Developed a strong understanding of financial market data, technical indicators, and risk management, learning innovative solutions for architectural challenges."
      },
      "techStack": {
        "frontend": "Vercel (optional for dashboard)",
        "backend": "Python, Google Sheets API",
        "tools": "GitHub Actions, Git, GitHub, .env, GitHub Secrets",
        "libraries": "XGBoost, CamemBERT (Hugging Face Transformers), Pandas, Requests, BeautifulSoup (implied)"
      },
      "learnings": [
        "Modular ML pipeline design significantly improves maintainability, debuggability, and scalability by breaking down complex workflows into distinct scripts.",
        "Leveraging services like Google Sheets and GitHub Actions for data storage and scheduling can drastically reduce operational overhead and development time, offering creative 'no-code' infrastructure solutions.",
        "For specialized tasks like financial news sentiment, utilizing domain-specific or language-specific transformer models (e.g., CamemBERT for French) is vital for high accuracy and relevant insights over generic NLP models.",
        "Regardless of prediction accuracy, a robust automated trading system *must* incorporate strict risk management rules (stop-loss, take-profit, position sizing) to protect capital and ensure long-term viability."
      ],
      "futureEnhancements": [
        "Integrate more cryptocurrencies, international stocks, or other asset classes to diversify the portfolio and expand trading opportunities.",
        "Add more financial news RSS feeds and explore web scraping for non-RSS sources to enrich the sentiment analysis with broader market coverage.",
        "Experiment with recurrent neural networks (RNNs) or more complex transformer architectures for enhanced time-series prediction or multi-modal data analysis.",
        "Implement dynamic position sizing or adaptive stop-loss/take-profit levels based on real-time market volatility or overall portfolio performance.",
        "Develop a sophisticated, interactive performance dashboard using Streamlit or Plotly Dash with real-time portfolio value and detailed trade analytics."
      ],
      "conclusion": "PublicTradeBot exemplifies integrating advanced ML, NLP, and smart automation to solve real financial challenges. It engineered a sophisticated, lean automated trading system, transforming raw news and market data into actionable, risk-managed decisions. Leveraging Python, GitHub Actions, and Google Sheets, this project showcases deep MLOps understanding, financial modeling, and the ability to build resilient, intelligent systems for efficient financial operations."
    },
    "SnakeBot": {
      "title": "Snake Bot",
      "description": "Intelligent AI agent mastering Snake using Dueling DQN and strategic planning.",
      "metadata": {
        "role": "Data Scientist",
        "category": "Reinforcement Learning - Game AI",
        "timeline": "December 12, 2025 - December 16, 2025",
        "liveUrl": null,
        "githubUrl": "https://github.com/Le-skal/SnakeBot"
      },
      "overview": "SnakeBot is an ambitious reinforcement learning project developing an AI to master the classic Snake game. This 'Enhanced Edition' uses a PyTorch-based Dueling Deep Q-Network and a rich 119-feature state representation with Hamiltonian cycle guidance. It features flexible training modes, demonstrating advanced AI principles for robust game agents.",
      "challenge": {
        "problem": "Classic Snake challenges AI with self-trapping and suboptimal moves reducing long-term survival. The core problem is learning efficient food acquisition while avoiding collisions and strategic cornering. High scores demand intelligent pathfinding and space management.",
        "goal": "The goal was to develop a robust AI that consistently achieves high scores in Snake by learning advanced, proactive strategies. This involved moving beyond reactive behaviors to incorporate spatial awareness and anticipatory planning. The aim was to demonstrate RL mastery in a dynamic environment.",
        "constraints": "Constraints included computational efficiency, requiring a lightweight network and training. Designing a comprehensive yet manageable 119-feature state representation was crucial. Engineering an effective reward function to guide long-term survival without local optima was also a challenge, alongside managing RL's inherently long training times."
      },
      "discovery": {
        "requirements": "Requirements centered on building an adaptive AI, necessitating a `SnakeEnv` for detailed game state, a `snake_ai.py` agent for decision-making, and `train.py` for learning. Different training modes (headless/visual) were needed for practical development workflows in RL.",
        "competitiveAnalysis": "The 'Enhanced Edition' and Dueling DQN choice imply awareness of existing Snake AI solutions and RL advancements. This aimed for superior performance over basic greedy or standard DQN approaches, integrating state-of-the-art value-based RL and hybrid strategies like Hamiltonian cycles.",
        "technicalResearch": "Research covered Deep Q-Networks, Dueling Network Architecture for stable learning, and non-greedy reward shaping for strategic play. Pathfinding algorithms like BFS and Hamiltonian Cycles were explored for state perception, alongside PyTorch best practices and Experience Replay for training stability."
      },
      "architecture": {
        "informationArchitecture": "The project employed a modular architecture: `game/` for environment logic and state, `training/` for the AI agent and learning process, and `algorithms/` for classical pathfinding methods. `main.py` served as the application entry point, orchestrating components for clarity and maintainability.",
        "technicalDecisions": "DQN, specifically Dueling DQN, was chosen for its success in game AI and improved value estimation. A comprehensive 119-feature state space, including danger sensors, grid representation, BFS, and Hamiltonian path guidance, was designed for informed decisions. A novel 'space preservation' reward structure, penalizing accessible space reduction, aimed for long-term survival. PyTorch provided flexibility for network development, while an interactive CLI enhanced training workflow."
      },
      "developmentProcess": {
        "phase1": "Phase 1 established core components: `game/environment.py` defined game rules and initial state. `training/snake_ai.py` structured the basic DQN agent, neural network layers, and initial reward structure.",
        "phase2": "Phase 2 focused on intelligence enhancement: The 'NEW REWARD STRUCTURE' for space preservation was implemented, and the state space expanded to 119 features, integrating BFS and Hamiltonian cycle guidance. `snake_ai.py` was upgraded to Dueling DQN, and `training/train.py` developed an interactive CLI for diverse training modes.",
        "phase3": "Phase 3 refined usability and performance. The interactive CLI was polished, and hyperparameters were likely tuned for optimal performance. Continuous consideration for a lightweight implementation ensured efficient operation during the intensive 4-day development."
      },
      "keyFeatures": [
        {
          "title": "Advanced Deep Q-Network (DQN) Agent with Dueling Architecture",
          "description": "The AI's core intelligence, it learns optimal actions from game states to maximize future rewards. Dueling DQN improves value estimation by separating state-value and advantage functions.",
          "implementation": "Implemented in `training/snake_ai.py` using PyTorch, the `DQN` class processes the 119-feature state, outputting state-value and advantage streams combined for Q-values. `torch.optim` updates weights, and `collections.deque` manages the experience replay buffer.",
          "challenges": "Overcoming challenges included designing an effective network for complex state, correctly implementing Dueling DQN, and managing experience replay for stable learning."
        },
        {
          "title": "Intelligent Reward Structure: Space Preservation",
          "description": "This novel mechanism guides the AI to prioritize open space, preventing cornering and self-trapping, not just seeking food. It encourages long-term survival strategies.",
          "implementation": "In `game/environment.py`, accessible space is calculated pre- and post-move. `space_reduction` results in a penalty (`-reduction * 0.5`), while `gaining space` provides a positive reward (`+abs(reduction) * 0.2`).",
          "challenges": "Challenges included balancing immediate food gain with long-term survival, tuning coefficients for effective behavior shaping, and efficient real-time accessible space calculation."
        },
        {
          "title": "Comprehensive 119-Feature State Representation",
          "description": "Provides the AI with a rich, detailed understanding of the game world, enabling more informed and strategic decision-making. It goes beyond simple sensory inputs for complex play.",
          "implementation": "Assembled in `game/environment.py`, the state vector combines immediate dangers, current/food directions, full grid representation, snake length, accessible space, BFS path to food, and crucial Hamiltonian path guidance with a `should_follow` flag.",
          "challenges": "Integrating diverse data types (binary, one-hot, normalized numerical, grid map) into a coherent vector was key. Ensuring all crucial information was encoded effectively without redundancy and scaled appropriately for the neural network were also challenges."
        },
        {
          "title": "Interactive Command-Line Interface (CLI) for Training",
          "description": "Offers a user-friendly way for developers to configure and choose different training modes. This simplifies running experiments and debugging the AI agent.",
          "implementation": "Implemented in `training/train.py`, the `interactive_menu()` displays options like 'Headless Training' for speed, 'Visual Training' for observation, and quick tests. User input directly controls training parameters and execution flow.",
          "challenges": "Challenges included designing an intuitive menu, robustly handling user input, and correctly dispatching to appropriate training functions based on user selections."
        },
        {
          "title": "Hamiltonian Cycle Pathfinding Integration",
          "description": "Provides the AI with a strategic 'guide' through a pre-calculated non-trapping path (Hamiltonian cycle) across the game grid. This is especially useful in later stages to prevent the snake from getting stuck.",
          "implementation": "The `algorithms/hamilton_cycle.py` generates the cycle. In `game/environment.py`, the AI's state vector includes one-hot encoded Hamiltonian path directions and a `should_follow` flag, directly informing decision-making.",
          "challenges": "Key challenges involved integrating a classical graph algorithm into an RL state space and determining conditions for the AI to prioritize following the Hamiltonian path versus exploring."
        }
      ],
      "testing": "Testing was integrated via `train.py` with `Quick Test (500 episodes, headless)` for sanity checks and `Quick Visual Demo (100 episodes)` for short feedback loops. 'Visual Training' provided crucial real-time debugging to observe AI behavior and identify issues. Iterative refinement, like the 'NEW REWARD STRUCTURE' and 'DUELING architecture' adoption, was driven by these observational tests.",
      "results": {
        "technicalAchievements": "Successfully implemented an advanced Dueling Deep Q-Network and crafted a sophisticated 119-feature state vector. Designed an innovative 'space preservation' reward structure for strategic long-term behavior. Developed a modular codebase and optimized performance for modest hardware, including headless training.",
        "businessImpact": "SnakeBot strongly demonstrates developer expertise in Python, PyTorch, RL, neural network design, and algorithm integration. It showcases the ability to tackle complex, dynamic problems with structured, data-driven approaches. The project provides a robust foundation for future game AI or control system work.",
        "personalGrowth": "Deepened understanding of Dueling DQNs, mastered reward function engineering and state space design. Gained practical experience integrating classical algorithms into RL, improving PyTorch proficiency and project structuring for complex AI systems."
      },
      "techStack": {
        "frontend": null,
        "backend": null,
        "tools": "Python, NumPy, collections.deque",
        "libraries": "PyTorch (torch, torch.nn, torch.optim) for deep learning; NumPy for numerical operations; collections.deque for experience replay."
      },
      "learnings": [
        "A comprehensive and intelligently engineered state space, like the 119-feature vector, is paramount to an AI's ability to learn complex strategies.",
        "Reward shaping, exemplified by the 'space preservation' structure, critically drives desired long-term behaviors and overcomes common RL pitfalls like self-trapping.",
        "Hybrid AI approaches, combining learned policies with classical algorithms like BFS and Hamiltonian cycles, significantly enhance agent capabilities and robustness.",
        "Practical development tools, such as an interactive CLI with headless and visual training modes, are vital for managing RL experiments and debugging effectively."
      ],
      "futureEnhancements": [
        "Explore more advanced RL algorithms like Prioritized Experience Replay, Rainbow DQN, or policy gradient methods (PPO) for faster and more stable learning.",
        "Implement automated hyperparameter tuning using tools like Optuna or Ray Tune to systematically optimize training parameters.",
        "Generalize the environment to support arbitrary grid dimensions, moving beyond fixed-size assumptions for Hamiltonian cycles and grid representation.",
        "Develop a polished web-based visualization or GUI to better showcase AI gameplay and training progress, increasing accessibility.",
        "Implement robust functionality for saving and loading trained models via the CLI, enabling easier evaluation and sharing of best-performing agents."
      ],
      "conclusion": "SnakeBot successfully applies reinforcement learning to the classic Snake game, showcasing sophisticated AI design. It combines Dueling DQN with an intricately engineered state representation and a novel space-preservation reward, creating an agent capable of advanced strategic play. This project highlights proficiency in PyTorch, ML principles, and pragmatic problem-solving, serving as a strong testament to the developer's ability to build complex AI systems."
    }
  }
}